import os
import json
import time
import logging
import requests
from tqdm import tqdm
from bs4 import BeautifulSoup
from concurrent.futures import ThreadPoolExecutor
from utils import row_aggregate

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏
URL = "https://m.mashina.kg/search/all/"
DATA_DIR = "data"
LINKS_FILE = f"{DATA_DIR}/links.json"
DATASET_FILE = f"{DATA_DIR}/dataset.jsonl"
REPORT_FILE = f"{DATA_DIR}/parsing_report.jsonl"
MAX_WORKERS = 10  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Ç–æ–∫–æ–≤

# –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é, –µ—Å–ª–∏ –µ–µ –Ω–µ—Ç
os.makedirs(DATA_DIR, exist_ok=True)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

def fetch_links(num_page):
    """–°–æ–±–∏—Ä–∞–µ—Ç —Å—Å—ã–ª–∫–∏ –Ω–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏—è –∏ –∑–∞–ø–∏—Å—ã–≤–∞–µ—Ç –≤ —Ñ–∞–π–ª."""
    os.environ["TZ"] = "Asia/Bishkek"
    time.tzset()
    start_time = time.strftime("%Y-%m-%d %H:%M:%S")
    new_links = set()

    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç–∞—Ä—ã–µ —Å—Å—ã–ª–∫–∏
    if os.path.exists(LINKS_FILE):
        with open(LINKS_FILE, "r") as f:
            try:
                saved_links = set(json.load(f))
            except json.JSONDecodeError:
                saved_links = set()
    else:
        saved_links = set()

    for page in tqdm(range(1, num_page + 1), desc="Fetching links"):
        try:
            page_url = f"{URL}?page={page}"
            response = requests.get(page_url, timeout=10)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, "lxml")

            for block_image in soup.find_all("div", class_="list-item list-label"):
                link = block_image.find("a")
                if link and "href" in link.attrs:
                    full_link = f"https://m.mashina.kg{link['href']}"
                    if full_link not in saved_links:
                        new_links.add(full_link)

        except requests.RequestException as e:
            logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page}: {e}")

    # –û–±–Ω–æ–≤–ª—è–µ–º —Ñ–∞–π–ª —Å—Å—ã–ª–æ–∫
    saved_links.update(new_links)
    with open(LINKS_FILE, "w") as f:
        json.dump(list(saved_links), f, indent=4)

    # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –æ—Ç—á–µ—Ç
    report = {
        "date": start_time,
        "end_time": time.strftime("%Y-%m-%d %H:%M:%S"),
        "new_links": len(new_links),
        "total_links": len(saved_links),
    }
    with open(REPORT_FILE, "a", encoding="utf-8") as f:
        f.write(json.dumps(report, ensure_ascii=False) + "\n")

    logging.info(f"‚úÖ –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω. –ù–∞–π–¥–µ–Ω–æ –Ω–æ–≤—ã—Ö —Å—Å—ã–ª–æ–∫: {len(new_links)}")
    return new_links

def fetch_offer(url):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –æ–±—ä—è–≤–ª–µ–Ω–∏—è."""
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, "lxml")

        row = row_aggregate(soup)
        row["link"] = url
        row["current_time"] = time.strftime("%Y-%m-%d %H:%M:%S")
        return json.dumps(row, ensure_ascii=False) + "\n"

    except requests.RequestException as e:
        logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {url}: {e}")
        return None

def fetch_offers():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –≤—Å–µ –æ–±—ä—è–≤–ª–µ–Ω–∏—è –ø–æ —Å–ø–∏—Å–∫—É —Å—Å—ã–ª–æ–∫, –ø—Ä–æ–ø—É—Å–∫–∞—è —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ."""
    os.environ["TZ"] = "Asia/Bishkek"
    time.tzset()

    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Å—ã–ª–∫–∏
    if not os.path.exists(LINKS_FILE):
        logging.error("–§–∞–π–ª links.json –Ω–µ –Ω–∞–π–¥–µ–Ω! –°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ fetch_links().")
        return

    with open(LINKS_FILE, "r") as f:
        sub_urls = json.load(f)

    # –ó–∞–≥—Ä—É–∂–∞–µ–º —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ —Å—Å—ã–ª–∫–∏
    processed_urls = set()
    if os.path.exists(DATASET_FILE):
        with open(DATASET_FILE, "r", encoding="utf-8") as f:
            for line in f:
                try:
                    data = json.loads(line)
                    processed_urls.add(data["link"])
                except json.JSONDecodeError:
                    continue  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –æ—à–∏–±–∫–∏

    # –§–∏–ª—å—Ç—Ä—É–µ–º —Å—Å—ã–ª–∫–∏ (–ø–∞—Ä—Å–∏–º —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã–µ)
    new_urls = [url for url in sub_urls if url not in processed_urls]
    
    if not new_urls:
        logging.info("‚úÖ –í—Å–µ –æ–±—ä—è–≤–ª–µ–Ω–∏—è —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã!")
        return

    logging.info(f"üîπ –ù–∞—á–∏–Ω–∞–µ–º –∑–∞–≥—Ä—É–∑–∫—É {len(new_urls)} –Ω–æ–≤—ã—Ö –æ–±—ä—è–≤–ª–µ–Ω–∏–π...")

    start_time = time.strftime("%Y-%m-%d %H:%M:%S")
    
    # –ú–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω–æ–µ —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        with open(DATASET_FILE, "a", encoding="utf-8") as f:
            for result in tqdm(executor.map(fetch_offer, new_urls), total=len(new_urls), desc="Processing offers"):
                if result:
                    f.write(result)

    # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –æ—Ç—á–µ—Ç
    report = {
        "date": start_time,
        "end_time": time.strftime("%Y-%m-%d %H:%M:%S"),
        "processed_links": len(new_urls),
        "total_links": len(sub_urls),
    }
    with open(REPORT_FILE, "a", encoding="utf-8") as f:
        f.write(json.dumps(report, ensure_ascii=False) + "\n")

    logging.info("‚úÖ –ü–∞—Ä—Å–∏–Ω–≥ –æ–±—ä—è–≤–ª–µ–Ω–∏–π –∑–∞–≤–µ—Ä—à–µ–Ω!")
